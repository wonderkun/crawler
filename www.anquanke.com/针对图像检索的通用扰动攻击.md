
# 针对图像检索的通用扰动攻击


                                阅读量   
                                **753453**
                            
                        |
                        
                                                                                    



##### 译文声明

本文是翻译文章，文章原作者Jie Li, Rongrong Ji, Hong Liu, Xiaopeng Hong, Yue Gao, Qi Tian，文章来源：
                                <br>原文地址：[https://arxiv.org/abs/1812.00552](https://arxiv.org/abs/1812.00552)

译文仅供参考，具体内容表达以及含义原文为准

[![](./img/200745/t011550f035ab871b3f.jpg)](./img/200745/t011550f035ab871b3f.jpg)



## 针对图像检索的通用扰动攻击

原文地址：[https://arxiv.org/abs/1812.00552](https://arxiv.org/abs/1812.00552)

通用对抗性扰动（UAP，**Universal adversarial perturbation**），也称为输入不可知扰动（**input-agnostic perturbation**），已被证明存在并且能够欺骗大量数据样本上最先进的深度学习模型。现有的UAP方法主要集中在攻击图像分类模型上。在本文中首次尝试攻击图像检索系统。

具体地说，图像检索攻击是使检索系统将不相关的图像返回到排名靠前的查询中。它对破坏图像检索攻击中特征之间的邻域关系起着重要作用。为此，提出了一种新颖的方法，即通过降级相应的排名指标来生成针对UAP的检索，以打破图像特征的邻域关系。

为了将攻击方法扩展到具有变化的输入大小或不可设涉及的网络参数的场景，提出了一种多尺度随机调整方案（**multi-scale random resizing**）和排序蒸馏策略（**ranking distillation strategy**）。在四个广泛使用的图像检索数据集上评估了该方法，并报告了不同指标（例如mAP和mP[@10](https://github.com/10)）的性能显着下降。最后，在实用的视觉搜索引擎（即Google Images）上测试了攻击方法，证明了方法的实际潜力。



## 0x01 Absert

卷积神经网络（CNN）已成为解决各种计算机视觉任务（例如图像分类，图像分割和目标检测）的最新解决方案。尽管取得了巨大的成功，但深度学习模型显示出容易受到输入图像的微小扰动的影响。

目前已经提出了各种攻击技术，例如模型蒸馏（**model distillation**），迁移学习（**transfer learning**）和梯度更新（**gradient updating**）。与以前称为图像特定扰动的方法相反，通用对抗扰动（UAP）可以使数据分布中的大多数图像被欺骗。作为通用，UAP可以方便地利用它来即时扰动看不见的数据点，而无需额外的计算。因此，UAP在广泛的应用中特别有用。

现有的方法无论是否与图像无关，都主要集中在图像分类上，而没有任何现有的工作触及攻击图像检索系统的工



作。作为计算机视觉中一个长期的研究主题，图像检索旨在从给定查询图像的数据集中找到相关图像。尽管在提高搜索准确性或效率有很多工作，但很少有人关注最新检索系统的漏洞。在图像检索中直接采用现有的UAP方法是困难的，原因来自四个方面：

**•不同的数据集标签格式：**设计用于图像分类的大多数现有UAP方法都适用于以类别标记的数据集，这需要UAP将数据点推过决策边界。但是，检索中的数据集通常用相似性标记，这需要UAP捕获特征之间的复杂关系。

**•不同的目标：**现有UAP方法的目标是干扰单个实例的一元和二分模型输出，例如，更改最可能的预测标签。但是，仅仅破坏top-1的结果还是不够的，因为检索评估通常是在排名列表上进行的。因此，对于攻击检索系统，应该通过降低准确样本的位置来干扰排名列表。

**•不同尺寸的模型输入：**通常现有UAP经过训练的模型要求输入图像的尺寸固定，但是这些UAP很脆弱，可以通过改变输入大小来防御。注意，检索中图像的大小通常是变化的，这限制了传统UAP的直接使用，因此对生成用于图像检索任务的UAP提出了更高的要求。

**•不同的模型输出和优化方法：**通常假定可以获取每个类别的预测置信度，并且置信度是一组连续和浮点数，它们快速响应输入的变化。它指示估计梯度以进行优化的方法。但是，检索系统返回的大规模离散排名列表几乎没有提供有关梯度近似的指导。这个事实使得将现有的UAP应用于无法访问网络参数的检索系统是不可行的。

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p5.ssl.qhimg.com/t01832ca81007a39a22.png)

在本文中首次尝试攻击图像检索，特别是部署在深度特征上的尖端图像检索模型。原则上目标是生成一个用于破坏特征空间中邻域关系的UAP，如上图所示。为解决上述挑战提出了一种新颖的通用对抗性摄动攻击方法来进行图像检索。详细地说，构建了一个通用模型来设计UAP，该模型通过稍微改变输入来打破特征点之间的邻域关系。首先通过基于最近和最远的组构造元组来考虑邻域结构之间的成对关系，通过在元组中交换相似关系来破坏这种关系。

尽管破坏成对的关系既简单又有效，但成对信息每次都专注于查询和两个数据样本之间的局部关系，而无需考虑对检索更重要的全局排名列表，认为它不能从根本上解决检索攻击问题。从理论上提出了从列表方面生成UAP的方法，该方法可以通过将相应的排名指标破坏到相对引用的较低位置来进一步排列整个排名列表。此外提出了一种多尺度随机调整大小方案，以将UAP应用于不同分辨率的输入图像，在实验上比固定尺度方法表现出更好的定位性能。所提出方法的流程如下图所示。

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p0.ssl.qhimg.com/t019c2d77f0b14a0a75.png)

方案还通过粗略到精细的策略进一步实现了攻击而无需触及网络参数，从而通过回归排名列表来提蒸馏模型，如下图所示。首先构造了粗略的子集，保留了从整个大型排名列表中选择采样的全局排名信息，并快速蒸馏模型以适应子集中的常规关系。然后从细粒度的层次上，集中于最相关的前k个实例进行检索以完善蒸馏的模型。

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p5.ssl.qhimg.com/t010b112e24a9439fc7.png)

从数量上讲，通用对抗性扰动可以使诸如mAP和mP @ 10的性能下降至少50％，这表明尖端的图像检索系统非常容易受到对抗性样本的攻击。进一步评估了在真实世界图像搜索引擎（即Google图片）上的通用扰动，并得出结论该扰动还会破坏输出排名列表。



## 0x02 Related Work

**对抗样本：**神经网络可以被对抗性样本所欺骗，一例如通过添加人眼难以感知的对抗性扰动。与可以访问受害人模型的“白盒”相反，“黑盒”是指攻击者对受害人知之甚少的情况。可以看出，为特定模型或训练集设计的扰动会欺骗其他模型和数据集，称为转移攻击，这在黑盒中被广泛采用。另一种流行的方法是知识蒸馏，它通过对受害者的输出进行回归获得替代模型，然后应用白盒攻击方法。

**检索的视觉特征：**图像检索是计算机视觉中一个长期存在的研究主题。给定查询图像，搜索引擎将从大量参考图像中检索相关的图像。典型设置是指在查询和参考之间提取和比较特征，例如全局描述符和局部描述符聚合。如今，最主要的检索方法主要基于CNN 。他们主要使用预先训练的CNN作来提取图像的整体表示。为此，经过ImageNet预先训练的CNN模型（例如AlexNet，VGGNet和ResNet）已经提供了优于人工获取特征的性能，微调CNN模型可以进一步提高检索性能。在这种趋势下，提出了许多最近的方法来构造可训练的合并层，以更好地表示特征。



## 0x03 The Proposed Method

旨在寻求约束为||δ||∞≤e的通用扰动δ，以破坏数据分布X中尽可能多的相似关系。这样，在添加了较小的扰动后，本来相似的特征应该会有所不同。为方便起见，用δ表示通用扰动，用以下方式表示第i个原始图像xi和对抗性特征向量的特征向量：

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p5.ssl.qhimg.com/t019062443ec609e4eb.png)

其中F（·）是通过CNN模型输出特征向量的函数，RI（·）和RP（·，·）分别是用于输入图像和通用摄动的调整大小运算符。 两个特征向量fi和fj之间的欧几里得距离被表征为函数d（fi，fj）。为了避免由大规模数据集引起的计算开销，预先计算将任何查询点与界标进行比较的基于界标(landmark-based)的顺序关系。



### <a class="reference-link" name="A.%E5%9F%BA%E5%87%86"></a>A.基准

首先尝试使用按标签的信息来干扰检索系统，以验证针对分类的UAP是否适合于检索。定义了一个分类器，该分类器配备了具有FC层和softmax层的交叉熵损失函数。将聚类索引识别为伪标签，并将其用于所有实验，以减少计算成本并确保每个实验均在相同的设置下进行。伪标签具有受害者模型的特征，其中包括比真实标签更多的受害者属性，并可能使攻击受益。此外，伪标签可以轻松地扩展到无法使用确切标签的场景。分类器通过伪标签进行训练，然后通过最大程度地减少广泛使用的分类攻击损失来欺骗，如下所示：

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p1.ssl.qhimg.com/t010971242da350d062.png)

其中[x] +是max（x，0）函数，Z（·）是softmax层之前的输出，t是纯输入的标签，x’是受δ干扰的输入。



### <a class="reference-link" name="B.UAP"></a>B.UAP

图像检索可以看作是一个排名问题，从这个角度来看，查询和引用之间的关系起着重要的作用。因此，应该充分利用这种关系，可以进一步提高攻击性能。为此认为两个关系被损坏，即，成对性和列表方式。

**损坏成对关系（Pair-wise）：**在这里使用最接近和最远参考之间的顺序关系来近似成对信息，可以直接通过经典三重态损失构造它们。形式化的，可以将有序关系集C编写如下：

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p0.ssl.qhimg.com/t01f654a9cb205ac4d1.png)

将ηik= 1定义为共享相同簇相似的xi和xk对。 ηij= 0表示对应于样本xi和xj的群集之间的距离最远。因此，可以重新计算属于C子集的一组元组。为了攻击检索系统，将传统的三元组损失降至最低，如下所示：

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p1.ssl.qhimg.com/t01dff54b19144aef5c.png)

其中，α是代表匹配和不匹配样本之间的余量的参数。

**损坏基于列表的关系（List-wise）：**与破坏仅基于本地关系的成对配对不同，进一步排列整个排名列表以实现逐列表关系，以破坏相应的排名指标。

由于列表通常太大而无法直接处理，因此重复使用以上采用的界标，并通过每次从每个界标中抽取参考来构造具有适当大小的排名列表的子集。将集群中心的反向排名列表视为理想的排名序列，并销毁标准化折扣累积增量（NDCG）度量，因为它是最适合信息检索的最经典度量。 NDCG是多级度量，旨在根据实例在结果列表中的位置来度量实例的收益。增益是从列表的顶部到底部累积的，较低级别的参考增益将被打折扣。给定集合S及其评级的任意集合g {yi} | g | i = 1，DCG定义如下：

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p5.ssl.qhimg.com/t011450cb4f8d4a234e.png)

NDCG将DCG除以理想排名序列的值，以确保范围为。但是，等式中的特征是非凸且非平滑的，这使得优化存在问题。为此，通过交换引用累积影响来近似梯度。给定查询图像特征fi按分数对图像进行排序后，如果yj和yk分别是当前第i个图像特征和第j个图像特征fj和fk的理想等级指数，则距离函数的切线为该属性如下：

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p0.ssl.qhimg.com/t019683a006138fc776.png)

因此，给定一个排名列表，可以直接计算方程中的梯度残差之和。 它大致接近等式中NDCG损失的梯度。 由于DCG中的折扣因子，还引入了λ参数来加权梯度残差，其梯度可以定义如下：

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p2.ssl.qhimg.com/t017a84fc513a4101f2.png)

其中|∆NDCGij|是NDCG度量的变化，如果交换第i和第j个引用的位置的话。



### <a class="reference-link" name="C.%E9%9A%8F%E6%9C%BA%E8%B0%83%E6%95%B4%E5%A4%A7%E5%B0%8F"></a>C.随机调整大小

与分类模型不同，分类模型将输入图像裁剪并填充为固定大小，而检索模型可以接受不同比例的输入。因此，调整大小是防御攻击的一种手段，它不仅影响检索性能，而且影响攻击质量。

为了使建议的通用扰动适合于不同的大小，采用了随机调整大小的过程RI(·)，它将大小为W×H×3的原始输入图像x调整为具有随机大小W’×H’×3的新图像RI(x) 。请注意，W’和H’在特定范围内，并且|W’W W H’H|应该在合理的小范围内以防止图像失真。然后，将UAPδ调整为新的扰动动RP(δ，RI(x))，其大小与RI（x）相同，以添加到输入图像中。



### <a class="reference-link" name="D.%E6%8E%92%E5%BA%8F%E8%92%B8%E9%A6%8F"></a>D.排序蒸馏

上述方法需要访问通常不现实的模型参数。为了克服它，提出了一种从粗到精的排序蒸馏方法来建立替代模型。注意，不同架构之间存在间隙，蒸馏也可以被视为有效的防御措施。因此，使用多种体系结构进行蒸馏可能无法正常工作。假设模型的架构是已知的。

由于对大型排名指数进行回归运算非常耗费计算量和内存，因此采用分层手法策略，该策略首先考虑粗粒度子集，然后重点关注细粒度的top-k引用。

对于粗粒度部分，考虑保留整个排名列表的子集，该子列表保留全局排名信息以供蒸馏模型回归。具体地，根据索引将大的排序列表划分为多个盒，并且通过从每个盒采样一个参考来构造子集。优化子集上的蒸馏模型以适合相应仓之间的序数关系。形式化的，顺序回归目标定义如下：

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p1.ssl.qhimg.com/t012d436c2b1c0cc525.png)

其中qi是第i个查询的蒸馏模型的特征，rim是第i个查询的子集中的第m个相似参考的特征，λm是确保顶级参考更重要的折扣因子，β是避免所有要素落入单个点的余量。

随后，对于细粒度部分，进行了针对前k个参考的改进过程。采用类似的策略作为参数减少（例如学习率和范围）减少的粗粒度部分。



### <a class="reference-link" name="E.%E4%BC%98%E5%8C%96"></a>E.优化

由于可以很容易地获得δ的梯度，因此采用具有动量的随机梯度下降来更新第i次迭代的扰动矢量：

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p1.ssl.qhimg.com/t014ec8b2b511aef80f.png)

其中gi是第i次迭代的动量，而λ是学习率。确保约束||δ||≤e的裁剪操作可能会在δ达到约束后使更新无效。当扰动饱和时，它将δ缩放为一半。详细的算法在下面中提供。

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p0.ssl.qhimg.com/t01ba3b038fb8987e64.png)



## 0x04 Experiments

在本节中提出定量结果和分析以评估所提出的攻击方案。使用了两个具有三个不同CNN模型（例如AlexNet ，VGGNet 和ResNet ）的最近基于CNN的图像描述符（即MAC和GeM ），形成了在120k的动态结构重构数据集上训练的六个CNN模型。使用Oxford5k和Paris6k及其修订版来评估攻击性能。

**训练数据集：**SfM数据集包含从Flickr下载的740万张图像。它分别包含两个名为SfM-30k和SfM-120k的大规模训练集。利用来自SfM-30k的6,403幅验证图像上的K-Means聚类获得列表式关系，并使用聚类索引作为伪标签来训练分类模型以获得标签式关系。对SfM-30k的1,691个查询图像进行了通用扰动训练。

**测试数据集：** Oxford5k数据集由5,062张图像组成，并且对该集合进行了手动注释，以生成11个不同实体地标的全面真实界标，每个界标由5个可能的查询表示。与Oxford5k相似，Paris6k数据集含6,412张图像和55个查询。将Revisited Oxford5k和Revisited Paris6k数据集分别称为ROxford5k和RParis6k。在原始数据集和修订后的数据集上报告结果。

**视觉特征：**对于基于CNN的图像表示，使用在ImageNet 上预先训练的AlexNet（A），VGG-16（V）ResNet101（R）[13]作为基本模型来进行微调SfM-120k数据集上的CNN模型。对于微调特征，考虑两个前沿特征，即广义均值池（GeM）和最大池（MAC）。结果获得了总共6个特征来评估攻击性能，分别称为A GeM，V-GeM，R-GeM，A-MAC，V-MAC和R-MAC。

**评估指标：**为了测量通用扰动的检索性能，主要考虑三个评估指标，即mAP，mP @ 10和欺骗率。与分类不同，top-1标签谓词的欺骗率不能直接用于图像检索。因此定义了一个新的指标来评估检索的欺骗率，称为下降率（DR），如下所示：

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p1.ssl.qhimg.com/t01074116d54a7233bb.png)

其中x是原始特征x的对抗示例，M是用于检索的度量，例如mAP。下降率通过测量检索系统的性能退化来表征攻击性能。下降率越高，攻击越成功。



### <a class="reference-link" name="A.UAP%E6%94%BB%E5%87%BB%E7%9A%84%E7%BB%93%E6%9E%9C"></a>A.UAP攻击的结果

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p3.ssl.qhimg.com/t01b1d9d1daeb954772.png)

评估了六个最先进的深层视觉表示对普遍对抗性干扰的性能，平均DR，mAP和mP @ 10的定量结果显示在上表中。 下降率低（VGG16除外）证明UAP抵抗检索分类的能力有限。尽管它们为VGG16取得了可观的结果，但仍比提出的方法差。显然，对于所有深层的视觉特征，所有通用扰动都在验证集上实现了很高的下降率。

大多数图像的降幅超过50％，这意味着大多数相关图像将不会返回到排名列表的顶部。具体而言，为V-MAC和R-GeM计算的通用扰动实现了近68％的下降率。列表式关系在产生普遍扰动中没有重要作用。将其归功于优化过程中采用的列表排名更高。得出结论，成对关系和按列表关系都适用于通用扰动生成，并且按列表关系可实现更好的性能。



### <a class="reference-link" name="B.%E8%BD%AC%E7%A7%BB%E6%94%BB%E5%87%BB%E7%9A%84%E7%BB%93%E6%9E%9C"></a>B.转移攻击的结果

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p5.ssl.qhimg.com/t0157d69b15ebd29963.png)

转移攻击是欺骗模型或数据集，并在另一个模型或数据集上产生扰动。上表显示了有关跨不同视觉特征的粘性转移的结果，其中报告了在所有四个评估数据集上计算的mDR。标签中的每一行。上面显示了给定模型制作的扰动mDR，每列显示了目标模型上的转移下降速率。通用扰动是在一种体系结构（例如V-GeM）上进行训练的，评估该体系结构的攻击能力是基于其他深层特征（例如R-MAC或V-MAC）来欺骗检索系统。有趣的是，从同一网络体系结构生成的通用扰动可以很好地转移到具有不同合并方法的相关模型。

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p4.ssl.qhimg.com/t0116cbfe99268f83a4.png)

还在测量蒸馏的能力。对于该架构是参见上表。很明显，尽管使用了相同的架构，但随机初始化模型的干扰却毫无意义。由于所有检索模型均已从ImageNet预训练模型中进行了微调，因此与其他架构的传递攻击相比，预训练模型产生的扰动可获得可观的结果。但是来自蒸馏模型的每个扰动比来自预训练模型的扰动至少高出6％，显示了对蒸馏进行排序的能力。得出的结论是，当模型参数无法被触及时，提出的排名分布攻击是可行的。



### <a class="reference-link" name="C.%E5%85%B3%E4%BA%8E%E8%B0%83%E6%95%B4%E5%A4%A7%E5%B0%8F%E7%9A%84%E6%95%88%E6%9E%9C"></a>C.关于调整大小的效果

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p3.ssl.qhimg.com/t0110eec75c4ff6a821.png)

如前所述，检索系统可以接受各种尺寸的输入图像，这激发了研究攻击系统时调整大小的效果。定量结果显示在上表。首先将调整大小比例设置为固定的362×362和1024×1024，考虑到362×362是用于训练检索模型的比例。 A-GeM和V-GeM的下降率低于多尺度随机调整大小方法的一半。最后，评估了范围对多尺度随机大小调整的影响，并观察到范围过大或过窄都会损害攻击性能。



### <a class="reference-link" name="D.%E5%8F%AF%E8%A7%86%E5%8C%96"></a>D.可视化

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p5.ssl.qhimg.com/t01a50f3bd512cb99df.png)

上图显示了从Oxbuild5k和Paris6K评估集中获取R-GeM特征的结果。在细节上，为了攻击标签关系，该模型旨在学习扰动以将原始图像推向其他类别。在第二行中，观察到检索到的前5张图像与狗的类别相关，而不是建筑物的真实类别。这种现象存在于成对关系和按列表关系，它们都在某种程度上追求最远的界标，例如，大多数检索到的图像与雕塑或油漆有关。注意，由于成对关系包括成对信息，所以成对关系和成对关系的检索图像相似。

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p0.ssl.qhimg.com/t014b419398f7102e3e.png)

然后在上图中可视化从不同模型训练的扰动。第一行的扰动是从MAC池生成的，第二行的扰动是从GeM池生成的。从不同网络生成的每行的前三个扰动表现出很大的差异，而来自同一列的扰动具有相似的外观。这与转移攻击的结果一致。此外，成对关系和列表关系所产生的扰动比标签关系所产生的扰动更相似，这也可能表明分类攻击和检索攻击之间存在差距。



### <a class="reference-link" name="E.%E7%9C%9F%E5%AE%9E%E7%B3%BB%E7%BB%9F%E6%94%BB%E5%87%BB"></a>E.真实系统攻击

[![](./img/200745/AAffA0nNPuCLAAAAAElFTkSuQmCC)](https://p1.ssl.qhimg.com/t01c61e2cc47589d3d9.png)

上图显示了在真实世界的图像检索系统（即Google Image）上的攻击结果。偶数行显示扰动的图像以及Google Image提供的检索到的图像和预测的关键字，这与原始行在奇数行完全不同。例如，原始输入被分类为单色，而对抗样本变为树。请注意，由于缺乏真实排名列表，它无法量化mAP下降。因此，对从Oxbuild5k和Paris6K数据集中随机采样的100张图像中，从损坏的图像的检索列表中删除从正确查询中检索到的图像的频率没有多少。对于此指标，模型实现了62.85％的缺席率。攻击结果表明，该方法可以产生通用扰动，以欺骗现实世界的搜索引擎。



## 0x05 Conclusion

本文提出了一套针对图像检索的通用攻击方法。 主要集中于攻击点式，成对和列表式邻居关系。 进一步详细分析了调整大小操作对产生普遍扰动的影响，并采用多尺度随机调整大小方法来提高上述攻击方案的成功率。 还提出了从粗到精的蒸馏策略以用于黑盒攻击。

在广泛使用的图像检索数据集（即Oxford5k和Paris6K）上评估了提出的方法，显示出很高的攻击性能，导致一系列模型中的大量检索指标下降。 最后，还攻击了真实世界的系统，即Google图片，进一步证明了方法的有效性。
